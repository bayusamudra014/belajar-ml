{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Sentimen Review Game\n",
    "\n",
    "Notebook ini berisi analisis sentimen menggunakan komentar review game. Data tersebut diambil dari Google Play Store sebagaimana yang dijelaskan pada `scrape.py`. Data diambil pada 31 Juli 2024. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Untuk menjalankan notebook ini, anda perlu menginstall dependensi berikut:\n",
    "1. Tensorflow\n",
    "2. Keras\n",
    "3. Python\n",
    "4. Numpy\n",
    "5. Nltk\n",
    "6. Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-02 10:38:04.886656: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-02 10:38:05.024769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-02 10:38:05.103618: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-02 10:38:05.115989: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-02 10:38:05.211383: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-02 10:38:06.158221: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from keras import losses\n",
    "from keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.17.0\n",
      "1.26.4\n",
      "3.8.1\n",
      "2.2.2\n",
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.version.full_version)\n",
    "print(nltk.__version__)\n",
    "print(pd.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut ini merupakan konstanta yang digunakan pada notebook ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"indonesian\"\n",
    "WORKER_NUMBER = 16\n",
    "WORD_EPOCH = 20\n",
    "INPUT_SIZE = 300\n",
    "\n",
    "LEARNING_EPOCH = 25\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pemrosesan Data\n",
    "Pada tahap ini, akan dilakukan proses pengolahan data. Pada tahap ini, data akan diimport. Stopword yang ada pada kalimat akan dihapus juga pada tahap ini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "regex = r'[^a-zA-Z0-9\\- \\n\"\\']+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loading\n",
    "\n",
    "Pada bagian ini, akan ditunjukan proses loading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Please dong yg game mininya, yang judul topeng...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seru sih cuman sayang banyak jawaban yg ndk co...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mantap Applikasinya bisa buat asah otak</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seruuuu</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sangat menarik untuk di mainkan</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score\n",
       "0  Please dong yg game mininya, yang judul topeng...      5\n",
       "1  Seru sih cuman sayang banyak jawaban yg ndk co...      3\n",
       "2            mantap Applikasinya bisa buat asah otak      5\n",
       "3                                            seruuuu      5\n",
       "4                    sangat menarik untuk di mainkan      4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score\n",
       "5    60159\n",
       "1    13760\n",
       "4     9250\n",
       "3     5335\n",
       "2     3140\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"score\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleansing\n",
    "\n",
    "Berikut ini merupakan proses pembersihan text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/miawheker/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/miawheker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(LANGUAGE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = []\n",
    "\n",
    "for d in data:\n",
    "    review = d[\"content\"]\n",
    "    result = []\n",
    "\n",
    "    for sentence in nltk.sent_tokenize(review):\n",
    "        sentence = sentence.lower()\n",
    "        words = sentence.split()\n",
    "\n",
    "        # Stopword removal\n",
    "        sentence = \" \".join([word for word in words if word not in stopwords])\n",
    "\n",
    "        # Remove special chars\n",
    "        sentence = re.sub(regex, '', sentence)\n",
    "        sentence = sentence.replace(\"-\", \" \")\n",
    "        sentence = sentence.replace(\"\\n\", \" \")\n",
    "        sentence = sentence.replace(\"\\\"\", \"\")\n",
    "        sentence = re.sub(r'\\bhttp[a-z0-9]+\\b', '', sentence)\n",
    "        sentence = re.sub(r'\\b.+@.+\\b', '', sentence)\n",
    "        sentence = re.sub(r'\\b(img|src)[a-z0-9]*\\b', '', sentence)\n",
    "        sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "        # Remove whitespaces\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        if len(sentence) > 0:\n",
    "            result.append(sentence)\n",
    "    \n",
    "\n",
    "    if len(result) > 0:\n",
    "        cleaned_data.append({\n",
    "            \"review\": result,\n",
    "            \"score\": d[\"score\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for d in cleaned_data:\n",
    "    for sentence in d[\"review\"]:\n",
    "        corpus.append(sentence)\n",
    "\n",
    "with open(\"data/corpus.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        if len(line) < 10:\n",
    "            continue\n",
    "\n",
    "        for sentence in nltk.sent_tokenize(line):\n",
    "            sentence = sentence.lower()\n",
    "            words = sentence.split()\n",
    "\n",
    "            # Stopword removal\n",
    "            sentence = \" \".join([word for word in words if word not in stopwords])\n",
    "\n",
    "            # Remove special chars\n",
    "            sentence = re.sub(regex, '', sentence)\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            sentence = sentence.replace(\"\\\"\", \"\")\n",
    "            sentence = re.sub(r'\\bhttp[a-z0-9]+\\b', '', sentence)\n",
    "            sentence = re.sub(r'\\b.+@.+\\b', '', sentence)\n",
    "            sentence = re.sub(r'\\b(img|src)[a-z0-9]*\\b', '', sentence)\n",
    "            sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "            # Remove whitespaces\n",
    "            sentence = sentence.strip()\n",
    "\n",
    "            if len(sentence) < 10:\n",
    "                continue\n",
    "\n",
    "            corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['please yg game mininya judul topeng makan lanjutin udah beres sampe level 500 sumpah candu banget game utamanya please',\n",
       " 'seru sih cuman sayang yg ndk cocok pertanyaannya petunjuknya tolong d perbaiki ya membingungkan',\n",
       " 'mantap applikasinya asah otak',\n",
       " 'seruuuu',\n",
       " 'menarik mainkan',\n",
       " 'permainan bagus berfikir',\n",
       " 'bagus mengasah otak',\n",
       " 'bagus mengasah otak',\n",
       " 'buset game nya keren banget suka banget main otak langsung pintar terimakasih yg gsme mengasah otak banget sih',\n",
       " 'yh bosen']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Vocab\n",
    "\n",
    "In this section, we try to calculate number of vocab that exist in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20875"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_vocab = len(vocab)\n",
    "number_of_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Pada tahap ini, akan dilakukan proses ekstraksi fitur. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Pada bagian ini, kita akan mencoba membuat TF-IDF layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722569891.561687   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.712966   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.719841   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.726826   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.739163   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.742179   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.893826   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.896394   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1722569891.898177   24612 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-02 10:38:11.899938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2117 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "layer_tfidf = keras.layers.TextVectorization(\n",
    "    max_tokens=number_of_vocab+1,\n",
    "    output_mode=\"tf_idf\",\n",
    "    split=\"whitespace\",\n",
    "    sparse=False,\n",
    "    pad_to_max_tokens=True,\n",
    "    ngrams=1\n",
    ")\n",
    "\n",
    "with tf.device(\"CPU\"):\n",
    "    layer_tfidf.adapt(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling\n",
    "\n",
    "Pada tahap ini, akan dilakukan proses melabeli dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_cnt = [0, 0, 0, 0, 0]\n",
    "\n",
    "for d in cleaned_data:\n",
    "    rating_cnt[d[\"score\"]-1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13550, 3100, 5240, 9050, 58052]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut ini merupakan pelabelan menggunakan skema dibawah. Pada skema ini, terdapat aturan sebagai berikut:\n",
    "* Rating < 3 dianggap memiliki sentimen negatif\n",
    "* Rating == 3 dianggap memiliki sentimen netral\n",
    "* Rating > 3 dianggap memiliki sentimen positif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.03015015, 12.80572519,  1.        ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data = []\n",
    "\n",
    "weight = np.array([rating_cnt[0] + rating_cnt[1], rating_cnt[2], rating_cnt[3] + rating_cnt[4]])\n",
    "max_weight = np.max(weight)\n",
    "weight = max_weight / weight\n",
    "\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in cleaned_data:\n",
    "  if d[\"score\"] < 3:\n",
    "    labelled_data.append([\" \".join(d[\"review\"]), [0,0,1], weight[0]])\n",
    "  elif d[\"score\"] == 3:\n",
    "    labelled_data.append([\" \".join(d[\"review\"]), [0,1,0], weight[1]])\n",
    "  else:\n",
    "    labelled_data.append([\" \".join(d[\"review\"]), [1,0,0], weight[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Pada tahap ini, akan dilakukan proses data splitting. Data akan dibagi menjadi 3 bagian, yaitu training data (80%), validation data (10%), dan test data (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88992"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cnt = len(labelled_data)\n",
    "dataset_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = labelled_data[:int(dataset_cnt * 0.8)]\n",
    "test_dataset = labelled_data[int(dataset_cnt * 0.8):int(dataset_cnt * 0.9)]\n",
    "validation_dataset = labelled_data[int(dataset_cnt * 0.9):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mari kita amati distribusi masing-masing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53742, 4191, 13260]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = [0, 0, 0]\n",
    "\n",
    "for d in train_dataset:\n",
    "    cnt[np.argmax(d[1])] += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6714, 511, 1674]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = [0, 0, 0]\n",
    "\n",
    "for d in test_dataset:\n",
    "    cnt[np.argmax(d[1])] += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6646, 538, 1716]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = [0, 0, 0]\n",
    "\n",
    "for d in validation_dataset:\n",
    "    cnt[np.argmax(d[1])] += 1\n",
    "\n",
    "cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Pada tahap ini, akan dilakukan training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self, dataset, repeat=1):\n",
    "        self.dataset = dataset\n",
    "        self.data_length = len(dataset)\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def generate(self):\n",
    "        for _ in range(self.repeat):\n",
    "          for x, y, w in self.dataset:\n",
    "                X = tf.convert_to_tensor([x], dtype=tf.string)\n",
    "                \n",
    "                Y = np.array([y], dtype=np.float32)\n",
    "                W = np.array([w], dtype=np.float32)\n",
    "                \n",
    "                yield X, Y, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Model\n",
    "\n",
    "Berikut ini adalah proses pembuatan model dengan TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"tfidf\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"tfidf\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20876</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,336,128</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20876\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,336,128\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,340,483</span> (5.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,340,483\u001b[0m (5.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,340,483</span> (5.11 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,340,483\u001b[0m (5.11 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 4ms/step - categorical_accuracy: 0.7311 - loss: 1.8819 - val_categorical_accuracy: 0.8767 - val_loss: 1.1962\n",
      "Epoch 2/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m253s\u001b[0m 4ms/step - categorical_accuracy: 0.8889 - loss: 1.1365 - val_categorical_accuracy: 0.9261 - val_loss: 0.9440\n",
      "Epoch 3/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4ms/step - categorical_accuracy: 0.9211 - loss: 0.9549 - val_categorical_accuracy: 0.9274 - val_loss: 0.8295\n",
      "Epoch 4/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 3ms/step - categorical_accuracy: 0.9272 - loss: 0.8660 - val_categorical_accuracy: 0.9385 - val_loss: 0.9303\n",
      "Epoch 5/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 4ms/step - categorical_accuracy: 0.9318 - loss: 0.8198 - val_categorical_accuracy: 0.9498 - val_loss: 0.7662\n",
      "Epoch 6/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m259s\u001b[0m 4ms/step - categorical_accuracy: 0.9371 - loss: 0.7687 - val_categorical_accuracy: 0.9491 - val_loss: 0.7570\n",
      "Epoch 7/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 3ms/step - categorical_accuracy: 0.9402 - loss: 0.7717 - val_categorical_accuracy: 0.9473 - val_loss: 0.6977\n",
      "Epoch 8/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 3ms/step - categorical_accuracy: 0.9395 - loss: 0.7567 - val_categorical_accuracy: 0.9561 - val_loss: 0.6536\n",
      "Epoch 9/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 4ms/step - categorical_accuracy: 0.9457 - loss: 0.7118 - val_categorical_accuracy: 0.9467 - val_loss: 0.6947\n",
      "Epoch 10/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 3ms/step - categorical_accuracy: 0.9457 - loss: 0.8015 - val_categorical_accuracy: 0.9653 - val_loss: 0.6272\n",
      "Epoch 11/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m248s\u001b[0m 3ms/step - categorical_accuracy: 0.9479 - loss: 0.7639 - val_categorical_accuracy: 0.9376 - val_loss: 0.6594\n",
      "Epoch 12/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m249s\u001b[0m 4ms/step - categorical_accuracy: 0.9462 - loss: 0.8150 - val_categorical_accuracy: 0.9487 - val_loss: 0.6263\n",
      "Epoch 13/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 4ms/step - categorical_accuracy: 0.9527 - loss: 0.7353 - val_categorical_accuracy: 0.9629 - val_loss: 0.6319\n",
      "Epoch 14/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 4ms/step - categorical_accuracy: 0.9482 - loss: 0.8737 - val_categorical_accuracy: 0.9660 - val_loss: 0.5671\n",
      "Epoch 15/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 4ms/step - categorical_accuracy: 0.9525 - loss: 0.7582 - val_categorical_accuracy: 0.9635 - val_loss: 0.5683\n",
      "Epoch 16/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 4ms/step - categorical_accuracy: 0.9561 - loss: 0.7831 - val_categorical_accuracy: 0.9392 - val_loss: 0.6281\n",
      "Epoch 17/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 4ms/step - categorical_accuracy: 0.9515 - loss: 0.8680 - val_categorical_accuracy: 0.9666 - val_loss: 0.6142\n",
      "Epoch 18/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 3ms/step - categorical_accuracy: 0.9520 - loss: 0.8297 - val_categorical_accuracy: 0.9529 - val_loss: 0.6125\n",
      "Epoch 19/25\n",
      "\u001b[1m71193/71193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 3ms/step - categorical_accuracy: 0.9560 - loss: 0.7120 - val_categorical_accuracy: 0.9445 - val_loss: 0.6089\n"
     ]
    }
   ],
   "source": [
    "train_gen = DataGenerator(train_dataset, repeat=LEARNING_EPOCH + 1)\n",
    "validation_gen = DataGenerator(validation_dataset, repeat=LEARNING_EPOCH + 1)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(train_gen.generate, output_signature=(\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "    tf.TensorSpec(shape=(None, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float64),\n",
    "))\n",
    "validation_ds = tf.data.Dataset.from_generator(validation_gen.generate, output_signature=(\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "    tf.TensorSpec(shape=(None, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float64),\n",
    "))\n",
    "\n",
    "callback = [\n",
    "  keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "  ),\n",
    "  keras.callbacks.ModelCheckpoint(\n",
    "    filepath=f\"models/checkpoint/tfidf_checkpoint.keras\",\n",
    "    save_best_only=True,\n",
    "  ),\n",
    "  keras.callbacks.TensorBoard(\n",
    "    log_dir=f\"logs/tfidf\",\n",
    "  ),\n",
    "]\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=(1,), dtype=tf.string),\n",
    "    layer_tfidf,\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(3, activation=\"softmax\"),\n",
    "  ],\n",
    "  name=f\"tfidf\",\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(\n",
    "  loss=losses.CategoricalCrossentropy(),\n",
    "  optimizer=optimizers.Adam(),\n",
    "  metrics=[keras.metrics.CategoricalAccuracy()],\n",
    ")\n",
    "model.fit(\n",
    "  train_ds,\n",
    "  validation_data=validation_ds,\n",
    "  epochs=LEARNING_EPOCH,\n",
    "  callbacks=callback,\n",
    "  steps_per_epoch=len(train_dataset),\n",
    "  validation_steps=len(validation_dataset),\n",
    ")\n",
    "model.save(f\"models/tfidf.keras\")\n",
    "\n",
    "tfidf_model = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil tersebut, terlihat bahwa model tfidf yang paling terbaik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluasi\n",
    "\n",
    "Pada bagian ini, kita akan mencoba melakukan evaluasi dari model yang telah dibuat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8899/8899\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1ms/step - categorical_accuracy: 0.9681 - loss: 0.5659\n",
      "Model accuracy: 0.9658388495445251\n"
     ]
    }
   ],
   "source": [
    "model = tfidf_model\n",
    "\n",
    "test_ds = DataGenerator(test_dataset, repeat=1)\n",
    "test_gen = tf.data.Dataset.from_generator(test_ds.generate, output_signature=(\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "    tf.TensorSpec(shape=(None, 3), dtype=tf.float32),\n",
    "    tf.TensorSpec(shape=(None,), dtype=tf.float64),\n",
    "))\n",
    "\n",
    "res = model.evaluate(test_gen, steps=len(test_dataset))\n",
    "print(f\"Model accuracy: {res[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dari hasil tersebut menunjukan bahwa model TF-IDF memiliki akurasi 95% pada dataset testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n",
    "\n",
    "Pada bagian ini, akan dilakukan demonstrasi terhadap model TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, sentence):\n",
    "    result = model.predict(tf.convert_to_tensor([sentence], dtype=tf.string))\n",
    "    argmax = np.argmax(result)\n",
    "\n",
    "    return [\"positif\", \"netral\", \"negatif\"][argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positif'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"Ih bagus banget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negatif'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"Game jelek banget\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'netral'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(model, \"main game dulu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
