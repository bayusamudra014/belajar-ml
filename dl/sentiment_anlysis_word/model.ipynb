{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Book Review Sentiment Model\n",
    "\n",
    "This notebook consists about building sentiment analysis model from Book Review comments. All datasets was scraped from Goodreads on July 2024. Goodreads was choosen because it may have good word and labeling quality. Scraped data format is line json (`.ljson`) which is a single of datum is in one line with format json. The reason I use this format is scalability issue. Scraping method is explained in `scrape.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To run this notebook, ensure that you have installed below dependencies:\n",
    "1. Tensorflow\n",
    "2. Keras\n",
    "3. Python\n",
    "4. Numpy\n",
    "5. Nltk\n",
    "6. Matplotlib\n",
    "7. Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 15:22:58.040620: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-31 15:22:58.323862: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-31 15:22:59.174674: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib as mt\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "from datetime import datetime\n",
    "from keras import losses\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below results show the version of the library that I used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.1\n",
      "1.26.4\n",
      "3.9.0\n",
      "1.5.0\n",
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.version.full_version)\n",
    "print(mt.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the constant of this notebook that I used for generating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "WORKER_NUMBER = 16\n",
    "\n",
    "LEARNING_EPOCH = 25\n",
    "PATIENCE = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing\n",
    "\n",
    "The first step of creating sentiment model is text preprocessing. In this step, I remove any unnecessary words, such as 'show more' and punctuations in text. I also do case folding to lowercase in this step. After text has been formatted, I stem every word using `nltk` tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANNED_KEYWORDS = [\n",
    "    \"This entire review has been hidden because of spoilers\",\n",
    "    \"hele kz kardei yok mu\",\n",
    "    \"ee bu hemen bitti\",\n",
    "]\n",
    "ENGLISH_THRESHOLD = 0.5\n",
    "corpus = []\n",
    "regex = r'[^a-zA-Z0-9\\- \\n\"\\']+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code shows text cleaning process and word tokenize. We save the result at `comments_cleaned.ljson` as cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /home/miawheker/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/miawheker/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(nltk.corpus.stopwords.words(LANGUAGE))\n",
    "english_words_data = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/comments_cleaned.ljson\", \"w\") as fw:\n",
    "    with open(\"datasets/comments.ljson\") as fr:\n",
    "        for line in fr:\n",
    "            data = json.loads(line)\n",
    "\n",
    "            # Skip if the comment contains banned keywords\n",
    "            if any(keyword in data[\"text\"] for keyword in BANNED_KEYWORDS):\n",
    "                continue\n",
    "\n",
    "            result = []\n",
    "            for sentence in nltk.sent_tokenize(data[\"text\"]):\n",
    "                # Case folding\n",
    "                sentence = sentence.lower()\n",
    "                words = sentence.split()\n",
    "\n",
    "                # Stopword removal\n",
    "                sentence = \" \".join([word for word in words if word not in stopwords])\n",
    "\n",
    "                # Remove special chars\n",
    "                sentence = re.sub(regex, '', sentence)\n",
    "                sentence = sentence.replace(\"-\", \" \")\n",
    "                sentence = sentence.replace(\"\\n\", \" \")\n",
    "                sentence = sentence.replace(\"\\\"\", \"\")\n",
    "                sentence = re.sub(r'\\bhttp[a-z0-9]+\\b', '', sentence)\n",
    "                sentence = re.sub(r'\\b(img|src)[a-z0-9]*\\b', '', sentence)\n",
    "                sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "                # Remove unnecessary words\n",
    "                sentence = sentence.replace(\"- - - - show more\", \"\")\n",
    "                sentence = sentence.replace(\"show more\", \"\")\n",
    "                sentence = sentence.replace(\"show less\", \"\")\n",
    "\n",
    "                # Remove whitespaces\n",
    "                sentence = sentence.strip()\n",
    "\n",
    "                if len(sentence) < 10:\n",
    "                    continue\n",
    "                \n",
    "                words = sentence.split()\n",
    "                \n",
    "                # Skip if the sentence is too short\n",
    "                wordlen = len(words)\n",
    "                if wordlen < 3:\n",
    "                    continue\n",
    "\n",
    "                english_cnt = 0\n",
    "                for word in words:\n",
    "                    if word in english_words_data:\n",
    "                        english_cnt += 1\n",
    "                \n",
    "                # Skip if the sentence contains too many non-english words\n",
    "                if english_cnt < ENGLISH_THRESHOLD * wordlen:\n",
    "                    continue\n",
    "\n",
    "                result.append(sentence)\n",
    "\n",
    "            # Change rating to integer\n",
    "            data[\"rating\"] = int(data[\"rating\"].split(\" \")[1])\n",
    "            data[\"text\"] = result\n",
    "\n",
    "            if len(data[\"text\"]) == 0:\n",
    "                continue\n",
    "\n",
    "            fw.write(json.dumps(data))\n",
    "            fw.write(\"\\n\")\n",
    "\n",
    "            corpus.extend(data[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code shows corpus enrichment that will used for word2vec datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/corpus.txt\", \"r\") as fr:\n",
    "    for line in fr:\n",
    "        if any(keyword in line for keyword in BANNED_KEYWORDS):\n",
    "            continue\n",
    "\n",
    "        if len(sentence) < 10:\n",
    "            continue\n",
    "\n",
    "        for sentence in nltk.sent_tokenize(line):\n",
    "            # Stopword removal\n",
    "            sentence = \" \".join([word for word in sentence.split() if word not in stopwords])\n",
    "\n",
    "            sentence = sentence.replace(\"\\n\", \" \")\n",
    "            sentence = re.sub(regex, '', line)\n",
    "            sentence = sentence.strip()\n",
    "            sentence = sentence.lower()\n",
    "\n",
    "            sentence = sentence.replace(\"-\", \" \")\n",
    "            sentence = sentence.replace(\"\\\"\", \"\")\n",
    "            sentence = re.sub(r'\\bhttp[a-z0-9]+\\b', '', sentence)\n",
    "            sentence = re.sub(r'\\b(img|src)[a-z0-9]*\\b', '', sentence)\n",
    "            sentence = re.sub(r'\\s{2,}', ' ', sentence)\n",
    "\n",
    "            sentence = sentence.replace(\"- - - - show more\", \"\")\n",
    "            sentence = sentence.replace(\"show more\", \"\")\n",
    "            sentence = sentence.replace(\"show less\", \"\")\n",
    "\n",
    "            if len(sentence) < 10:\n",
    "                continue\n",
    "\n",
    "            words = sentence.split()\n",
    "\n",
    "            wordlen = len(words)\n",
    "            if wordlen < 3:\n",
    "                continue\n",
    "\n",
    "            english_cnt = 0\n",
    "\n",
    "            for word in words:\n",
    "                if word in english_words_data:\n",
    "                    english_cnt += 1\n",
    "\n",
    "            if english_cnt < ENGLISH_THRESHOLD * wordlen:\n",
    "                continue\n",
    "            \n",
    "            if sentence not in corpus:\n",
    "                corpus.append(sentence)\n",
    "\n",
    "with open(\"datasets/corpus_cleaned.txt\", \"w\") as fw:\n",
    "    fw.write(\"\\n\".join(corpus))\n",
    "\n",
    "# Cleanup\n",
    "corpus = []\n",
    "stopwords = []\n",
    "english_words = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code shows stemming process on corpus and datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is process of stemming process on corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/corpus_stemmed.txt\", \"w\") as fw:\n",
    "  with open(\"datasets/corpus_cleaned.txt\", \"r\") as fr:\n",
    "      for line in fr:\n",
    "          words = nltk.word_tokenize(line, language=LANGUAGE)\n",
    "          result = \" \".join([stemmer.stem(word) for word in words])\n",
    "\n",
    "          fw.write(result)\n",
    "          fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets/comments_stemmed.ljson\", \"w\") as fw:\n",
    "  with open(\"datasets/comments_cleaned.ljson\", \"r\") as fr:\n",
    "      for line in fr:\n",
    "          data = json.loads(line)\n",
    "          result = []\n",
    "          \n",
    "          for sentence in data[\"text\"]:\n",
    "              words = nltk.word_tokenize(sentence, language=LANGUAGE)\n",
    "              sentence = \" \".join([stemmer.stem(word) for word in words])\n",
    "              result.append(sentence)\n",
    "\n",
    "          fw.write(json.dumps({\n",
    "              \"text\": result,\n",
    "              \"rating\": data[\"rating\"]\n",
    "          }))\n",
    "          fw.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "stemmer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "This section will explain about feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceIterator:\n",
    "    \"\"\"This class is used to stream all line over the file.\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                yield line.split()\n",
    "\n",
    "class LineIterator:\n",
    "    \"\"\"This class is used to stream all line over the file.\"\"\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def generate(self):\n",
    "        with open(self.filename, \"r\") as f:\n",
    "            for line in f:\n",
    "                yield line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Vocab\n",
    "\n",
    "In this section, we try to calculate number of vocab that exist in our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "\n",
    "for sentence in SentenceIterator(\"datasets/corpus_stemmed.txt\"):\n",
    "    for word in sentence:\n",
    "        vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83295"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_vocab = len(vocab)\n",
    "number_of_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization Layer\n",
    "\n",
    "In this section, there is a function that we use to generate feature extraction layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_extraction(output_mode, datasets=\"datasets/corpus_stemmed.txt\",*, sparse=True, pad_to_max_tokens=True):\n",
    "        it = LineIterator(datasets)\n",
    "        ds = tf.data.Dataset.from_generator(it.generate, output_signature=tf.TensorSpec(shape=(), dtype=tf.string))\n",
    "\n",
    "        layer = keras.layers.TextVectorization(\n",
    "            max_tokens=number_of_vocab+1,\n",
    "            output_mode=output_mode,\n",
    "            split=\"whitespace\",\n",
    "            sparse=sparse,\n",
    "            pad_to_max_tokens=pad_to_max_tokens,\n",
    "            ngrams=1\n",
    "        )\n",
    "        \n",
    "        with tf.device(\"CPU\"):\n",
    "            layer.adapt(ds)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-31 15:47:14.310635: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.350672: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.361081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.379380: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.384414: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.389575: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.539766: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.542507: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.544716: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-31 15:47:14.549692: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2125 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2024-07-31 16:45:14.191323: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TextVectorization name=text_vectorization, built=False>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = generate_text_extraction(\"tf_idf\", sparse=False)\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(83296,), dtype=float32, numpy=\n",
       "array([11.75045 ,  1.883836,  0.      , ...,  0.      ,  0.      ,\n",
       "        0.      ], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tfidf layer\n",
    "tfidf(\"i love this book, love it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = keras.layers.TextVectorization(\n",
    "            max_tokens=number_of_vocab+1,\n",
    "            output_mode=\"count\",\n",
    "            split=\"whitespace\",\n",
    "            sparse=False,\n",
    "            vocabulary=[word for word in vocab],\n",
    "            ngrams=1\n",
    "        )\n",
    "vocab = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(83296,), dtype=int64, numpy=array([1, 0, 0, ..., 0, 0, 0])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the bow layer\n",
    "bow(\"i love this book, love it\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "In this section, we will learn about our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data: 46853\n",
      "Rating distribution: [2272, 4629, 8451, 15807, 15694]\n"
     ]
    }
   ],
   "source": [
    "data_cnt = 0\n",
    "rating = [0,0,0,0,0]\n",
    "\n",
    "with open(\"datasets/comments_stemmed.ljson\", \"r\") as fr:\n",
    "    for line in fr:\n",
    "        data_cnt += 1\n",
    "\n",
    "        data = json.loads(line)\n",
    "        rating[data[\"rating\"] - 1] += 1\n",
    "\n",
    "print(\"Number of data:\", data_cnt)\n",
    "print(\"Rating distribution:\", rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling and Data Split\n",
    "\n",
    "In this section, we will label `comments_stemmed` dataset. We will use two scheme of based on rating. Sentiment classification will devide into 3 class, positive (index 0), neutral (index 1), and negative (index 2). The label will be a vector with three element with range value between 0 and 1. This label is a softmax of three class that we defined before.\n",
    "\n",
    "The dataset will be splitted into 3 parts, training with proportion 70% of data, test with proportion 15% of data, and validation with proportion of 15% data.\n",
    "\n",
    "This cells will classify the type of data, whether it is a training data, test data, or validation data randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = np.zeros((data_cnt,), dtype=np.int8)\n",
    "\n",
    "for i in range(0, int(data_cnt * 0.70)):\n",
    "    data_type[i] = 0\n",
    "\n",
    "for i in range(int(data_cnt * 0.70), int(data_cnt * 0.85 + 1)):\n",
    "    data_type[i] = 1\n",
    "\n",
    "for i in range(int(data_cnt * 0.85 + 1), data_cnt):\n",
    "    data_type[i] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 1, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle data\n",
    "np.random.shuffle(data_type)\n",
    "data_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type distribution: [32797  7029  7027]\n"
     ]
    }
   ],
   "source": [
    "data_type_dist = np.bincount(data_type)\n",
    "\n",
    "print(\"Data type distribution:\", data_type_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_rating_dist = np.zeros((5,), dtype=np.int32)\n",
    "\n",
    "idx = 0\n",
    "with open(\"datasets/comments_stemmed.ljson\", \"r\") as fr:\n",
    "    for line in fr:\n",
    "        data = json.loads(line)\n",
    "\n",
    "        if data_type[idx] == 0:\n",
    "            training_rating_dist[data[\"rating\"] - 1] += 1\n",
    "        \n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rating distribution: [ 1558  3263  5886 10945 11145]\n"
     ]
    }
   ],
   "source": [
    "print(\"Training rating distribution:\", training_rating_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheme 1\n",
    "\n",
    "In this scheme, we will label data training with this rules:\n",
    "1. Star 5 will be labelled as `[1, 0, 0]`\n",
    "2. Star 4 will be labelled as `[1, 0, 0]`\n",
    "3. Star 3 will be labelled as `[0, 1, 0]`\n",
    "4. Star 2 will be labelled as `[0, 0, 1]`\n",
    "5. Star 1 will be labelled as `[0, 0, 1]`\n",
    "\n",
    "Those labels are based on the highest element value in resulted vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.1534018 , 3.4155685 , 1.89347604, 1.01827318, 1.        ])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calclate data weight\n",
    "high_dist = training_rating_dist[0] + training_rating_dist[1]\n",
    "low_dist = training_rating_dist[3] + training_rating_dist[4]\n",
    "center_dist = training_rating_dist[2]\n",
    "\n",
    "max_dist = max(high_dist, low_dist, center_dist)\n",
    "\n",
    "data_weight = np.ones((5,), dtype=np.float64) * max_dist\n",
    "data_weight = data_weight / np.array([high_dist, high_dist, center_dist, low_dist, low_dist])\n",
    "data_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "with open(\"datasets/comments_stemmed.ljson\", \"r\") as fr:\n",
    "    with open(\"datasets/comments_labelled_s1_train.ljson\", \"w\") as ftrain, \\\n",
    "         open(\"datasets/comments_labelled_s1_val.ljson\", \"w\") as fval, \\\n",
    "         open(\"datasets/comments_labelled_s1_test.ljson\", \"w\") as ftest:\n",
    "        for line in fr:\n",
    "            data = json.loads(line)\n",
    "            y = []\n",
    "\n",
    "            if data[\"rating\"] == 1:\n",
    "                y = [1, 0, 0]\n",
    "            elif data[\"rating\"] == 2:\n",
    "                y = [1, 0, 0]\n",
    "            elif data[\"rating\"] == 3:\n",
    "                y = [0, 1, 0]\n",
    "            elif data[\"rating\"] == 4:\n",
    "                y = [0, 0, 1]\n",
    "            elif data[\"rating\"] == 5:\n",
    "                y = [0, 0, 1]\n",
    "                        \n",
    "            write_data = {\n",
    "                \"X\": \" \".join(data[\"text\"]),\n",
    "                \"y\": y,\n",
    "                \"w\": data_weight[data[\"rating\"] - 1]\n",
    "            }\n",
    "\n",
    "            write_data = json.dumps(write_data)\n",
    "\n",
    "            if data_type[idx] == 0:\n",
    "                ftrain.write(write_data)\n",
    "                ftrain.write(\"\\n\")\n",
    "            elif data_type[idx] == 1:\n",
    "                fval.write(write_data)\n",
    "                fval.write(\"\\n\")\n",
    "            elif data_type[idx] == 2:\n",
    "                ftest.write(write_data)\n",
    "                ftest.write(\"\\n\")\n",
    "            \n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheme 2\n",
    "\n",
    "In this scheme, we will label data training with this rules:\n",
    "1. Star 5 will be labelled as `[1]`\n",
    "2. Star 4 will be labelled as `[1]`\n",
    "3. Star 3 will be labelled as `[1]`\n",
    "4. Star 2 will be labelled as `[0]`\n",
    "5. Star 1 will be labelled as `[0]`\n",
    "\n",
    "Those labels are based on the highest element value in resulted vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.58203692, 4.58203692, 3.75297316, 1.        , 1.        ])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate data weight\n",
    "high_dist = training_rating_dist[0] + training_rating_dist[1] + training_rating_dist[2]\n",
    "low_dist = training_rating_dist[3] + training_rating_dist[4]\n",
    "\n",
    "max_dist = max(high_dist, low_dist)\n",
    "\n",
    "data_weight = np.ones((5,), dtype=np.float64) * max_dist\n",
    "data_weight = data_weight / np.array([high_dist, high_dist, high_dist, low_dist, low_dist])\n",
    "data_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "with open(\"datasets/comments_stemmed.ljson\", \"r\") as fr:\n",
    "    with open(\"datasets/comments_labelled_s2_train.ljson\", \"w\") as ftrain, \\\n",
    "         open(\"datasets/comments_labelled_s2_val.ljson\", \"w\") as fval, \\\n",
    "         open(\"datasets/comments_labelled_s2_test.ljson\", \"w\") as ftest:\n",
    "        for line in fr:\n",
    "            data = json.loads(line)\n",
    "            y = []\n",
    "\n",
    "            if data[\"rating\"] == 1:\n",
    "                y = [1]\n",
    "            elif data[\"rating\"] == 2:\n",
    "                y = [1]\n",
    "            elif data[\"rating\"] == 3:\n",
    "                y = [1]\n",
    "            elif data[\"rating\"] == 4:\n",
    "                y = [0]\n",
    "            elif data[\"rating\"] == 5:\n",
    "                y = [0]\n",
    "                        \n",
    "            write_data = {\n",
    "                \"X\": \" \".join(data[\"text\"]),\n",
    "                \"y\": y,\n",
    "                \"w\": data_weight[data[\"rating\"] - 1]\n",
    "            }\n",
    "\n",
    "            write_data = json.dumps(write_data)\n",
    "\n",
    "            if data_type[idx] == 0:\n",
    "                ftrain.write(write_data)\n",
    "                ftrain.write(\"\\n\")\n",
    "            elif data_type[idx] == 1:\n",
    "                fval.write(write_data)\n",
    "                fval.write(\"\\n\")\n",
    "            elif data_type[idx] == 2:\n",
    "                ftest.write(write_data)\n",
    "                ftest.write(\"\\n\")\n",
    "            \n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model Training\n",
    "\n",
    "In this section, we will explore about training model for sentiment analysis. The hyperparameters are below:\n",
    "1. Word Model:\n",
    "   We will use `word2vec_model` and `fasttext_model` based on model that we have trained before.\n",
    "2. Classification Model:\n",
    "   We will use one type of layer. \n",
    "3. Dataset Scheme:\n",
    "   We will use two scheme that we have build before, that is `Sheme 1` and `Scheme 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below cell is helper function to do our task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedGenerator:\n",
    "    def __init__(self, filename, word_model, repeat=1):\n",
    "        self.filename = filename\n",
    "        self.word_model = word_model\n",
    "        self.repeat = repeat\n",
    "\n",
    "    def generate(self):\n",
    "        for _ in range(self.repeat):\n",
    "          with open(self.filename, \"r\") as f:\n",
    "              for line in f:\n",
    "                  data = json.loads(line)\n",
    "                  X = tf.convert_to_tensor([data[\"X\"]], dtype=tf.string)\n",
    "                  \n",
    "                  y = np.array([data[\"y\"]], dtype=np.float32)\n",
    "                  w = np.array([data[\"w\"]], dtype=np.float32)\n",
    "                  \n",
    "                  yield X, y, w\n",
    "\n",
    "def train_model(*, word, classification, train_dataset_filename, validation_dataset_filename, name):\n",
    "    train_gen = EmbedGenerator(train_dataset_filename, word, repeat=LEARNING_EPOCH)\n",
    "    validation_gen = EmbedGenerator(validation_dataset_filename, word, repeat=LEARNING_EPOCH)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(train_gen.generate, output_signature=(\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float64),\n",
    "    ))\n",
    "    validation_ds = tf.data.Dataset.from_generator(validation_gen.generate, output_signature=(\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.string),\n",
    "        tf.TensorSpec(shape=(None, 1), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.float64),\n",
    "    ))\n",
    "\n",
    "    callback = [\n",
    "      keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        min_delta=0.001,\n",
    "      ),\n",
    "      keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"models/checkpoint/{name}_model_checkpoint.keras\",\n",
    "        save_best_only=True,\n",
    "      ),\n",
    "      keras.callbacks.TensorBoard(\n",
    "        log_dir=f\"logs/{name}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\",\n",
    "      ),\n",
    "    ]\n",
    "\n",
    "    print(\"generate text extraction layer\")\n",
    "    extraction_layer = word\n",
    "\n",
    "    print(\"generate model\")\n",
    "    if classification == \"layer_1\":\n",
    "      model = keras.models.Sequential([\n",
    "          keras.layers.Input(shape=(1,), dtype=tf.string),\n",
    "          extraction_layer,\n",
    "          keras.layers.Dense(64, activation=\"relu\"),\n",
    "          keras.layers.Dropout(0.3),\n",
    "          keras.layers.Dense(64, activation=\"relu\"),\n",
    "          keras.layers.Dropout(0.3),\n",
    "          keras.layers.Dense(64, activation=\"relu\"),\n",
    "          keras.layers.Dropout(0.3),\n",
    "          keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "        ],\n",
    "        name=name,\n",
    "      )\n",
    "    else:\n",
    "      raise ValueError(\"Invalid classification\")\n",
    "    \n",
    "    model.summary()\n",
    "    model.compile(\n",
    "      loss=losses.BinaryCrossentropy(),\n",
    "      optimizer=optimizers.Adam(),\n",
    "      metrics=[keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "      train_ds,\n",
    "      epochs=LEARNING_EPOCH,\n",
    "      callbacks=callback,\n",
    "      validation_data=validation_ds,\n",
    "      batch_size=32,\n",
    "      steps_per_epoch=data_type_dist[0],\n",
    "      validation_steps=data_type_dist[1],\n",
    "    )\n",
    "\n",
    "    model.save(f\"models/{name}_model.keras\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = [\n",
    "  (\"tfidf\", tfidf),\n",
    "  (\"bow\", bow),\n",
    "]\n",
    "\n",
    "classification_scheme = [\n",
    "  (\"layer1\", \"layer_1\"), \n",
    "]\n",
    "\n",
    "dataset = [\n",
    "  (\"schema1\",\"datasets/comments_labelled_s1_train.ljson\", \"datasets/comments_labelled_s1_val.ljson\", \"datasets/comments_labelled_s1_test.ljson\"),\n",
    "  (\"schema2\",\"datasets/comments_labelled_s2_train.ljson\", \"datasets/comments_labelled_s2_val.ljson\", \"datasets/comments_labelled_s2_test.ljson\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training schema1_tfidf_layer1 model\n",
      "generate text extraction layer\n",
      "generate model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"schema1_tfidf_layer1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"schema1_tfidf_layer1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83296</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,331,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83296\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m5,331,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 8ms/step - binary_accuracy: 0.2736 - loss: 1.1508 - val_binary_accuracy: 0.3548 - val_loss: 1.0247\n",
      "Epoch 2/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 8ms/step - binary_accuracy: 0.3411 - loss: 1.0185 - val_binary_accuracy: 0.3552 - val_loss: 1.0176\n",
      "Epoch 3/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 8ms/step - binary_accuracy: 0.3488 - loss: 0.9720 - val_binary_accuracy: 0.3538 - val_loss: 1.0272\n",
      "Epoch 4/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m268s\u001b[0m 8ms/step - binary_accuracy: 0.3550 - loss: 0.9215 - val_binary_accuracy: 0.3561 - val_loss: 1.0212\n",
      "Epoch 5/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 9ms/step - binary_accuracy: 0.3564 - loss: 0.9033 - val_binary_accuracy: 0.3565 - val_loss: 1.0100\n",
      "Epoch 6/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 7ms/step - binary_accuracy: 0.3578 - loss: 0.8825 - val_binary_accuracy: 0.3558 - val_loss: 1.0260\n",
      "Epoch 7/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m205s\u001b[0m 6ms/step - binary_accuracy: 0.3598 - loss: 0.8601 - val_binary_accuracy: 0.3555 - val_loss: 1.0516\n",
      "Epoch 8/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 7ms/step - binary_accuracy: 0.3628 - loss: 0.8461 - val_binary_accuracy: 0.3562 - val_loss: 1.0667\n",
      "Epoch 9/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m257s\u001b[0m 8ms/step - binary_accuracy: 0.3606 - loss: 0.8466 - val_binary_accuracy: 0.3564 - val_loss: 1.1156\n",
      "Epoch 10/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 8ms/step - binary_accuracy: 0.3622 - loss: 0.8407 - val_binary_accuracy: 0.3552 - val_loss: 1.0892\n",
      "\n",
      "Training schema1_bow_layer1 model\n",
      "generate text extraction layer\n",
      "generate model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"schema1_bow_layer1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"schema1_bow_layer1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83296</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,331,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83296\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m5,331,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 8ms/step - binary_accuracy: 0.3024 - loss: 1.0953 - val_binary_accuracy: 0.3515 - val_loss: 0.9861\n",
      "Epoch 2/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 8ms/step - binary_accuracy: 0.3452 - loss: 0.9862 - val_binary_accuracy: 0.3538 - val_loss: 1.0156\n",
      "Epoch 3/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 8ms/step - binary_accuracy: 0.3515 - loss: 0.9476 - val_binary_accuracy: 0.3560 - val_loss: 1.0410\n",
      "Epoch 4/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m262s\u001b[0m 8ms/step - binary_accuracy: 0.3550 - loss: 0.9150 - val_binary_accuracy: 0.3561 - val_loss: 1.0051\n",
      "Epoch 5/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 8ms/step - binary_accuracy: 0.3590 - loss: 0.8884 - val_binary_accuracy: 0.3572 - val_loss: 1.0552\n",
      "Epoch 6/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 8ms/step - binary_accuracy: 0.3602 - loss: 0.8729 - val_binary_accuracy: 0.3567 - val_loss: 1.0423\n",
      "\n",
      "Training schema2_tfidf_layer1 model\n",
      "generate text extraction layer\n",
      "generate model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"schema2_tfidf_layer1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"schema2_tfidf_layer1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83296</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,331,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83296\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m5,331,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m270s\u001b[0m 8ms/step - binary_accuracy: 0.5831 - loss: 1.3610 - val_binary_accuracy: 0.7435 - val_loss: 1.1905\n",
      "Epoch 2/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m269s\u001b[0m 8ms/step - binary_accuracy: 0.7182 - loss: 1.1591 - val_binary_accuracy: 0.7433 - val_loss: 1.1815\n",
      "Epoch 3/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 8ms/step - binary_accuracy: 0.7470 - loss: 1.0906 - val_binary_accuracy: 0.7442 - val_loss: 1.2107\n",
      "Epoch 4/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m271s\u001b[0m 8ms/step - binary_accuracy: 0.7613 - loss: 1.0158 - val_binary_accuracy: 0.7389 - val_loss: 1.2850\n",
      "Epoch 5/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 8ms/step - binary_accuracy: 0.7699 - loss: 1.0058 - val_binary_accuracy: 0.7470 - val_loss: 1.2372\n",
      "Epoch 6/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 8ms/step - binary_accuracy: 0.7754 - loss: 0.9459 - val_binary_accuracy: 0.7453 - val_loss: 2.1553\n",
      "Epoch 7/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 8ms/step - binary_accuracy: 0.7794 - loss: 0.9223 - val_binary_accuracy: 0.7441 - val_loss: 1.4337\n",
      "\n",
      "Training schema2_bow_layer1 model\n",
      "generate text extraction layer\n",
      "generate model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"schema2_bow_layer1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"schema2_bow_layer1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">83296</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,331,008</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m83296\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_16 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m5,331,008\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_13 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_14 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,339,393</span> (20.37 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,339,393\u001b[0m (20.37 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 8ms/step - binary_accuracy: 0.6157 - loss: 1.2889 - val_binary_accuracy: 0.7455 - val_loss: 1.2264\n",
      "Epoch 2/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m274s\u001b[0m 8ms/step - binary_accuracy: 0.7176 - loss: 1.1050 - val_binary_accuracy: 0.7490 - val_loss: 1.1915\n",
      "Epoch 3/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m275s\u001b[0m 8ms/step - binary_accuracy: 0.7500 - loss: 1.0321 - val_binary_accuracy: 0.7433 - val_loss: 1.1820\n",
      "Epoch 4/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m278s\u001b[0m 8ms/step - binary_accuracy: 0.7597 - loss: 0.9733 - val_binary_accuracy: 0.7505 - val_loss: 1.2616\n",
      "Epoch 5/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m279s\u001b[0m 9ms/step - binary_accuracy: 0.7705 - loss: 0.9178 - val_binary_accuracy: 0.7456 - val_loss: 1.3664\n",
      "Epoch 6/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 9ms/step - binary_accuracy: 0.7755 - loss: 0.8919 - val_binary_accuracy: 0.7452 - val_loss: 1.6120\n",
      "Epoch 7/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m283s\u001b[0m 9ms/step - binary_accuracy: 0.7804 - loss: 0.8541 - val_binary_accuracy: 0.7412 - val_loss: 1.4857\n",
      "Epoch 8/25\n",
      "\u001b[1m32797/32797\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m284s\u001b[0m 9ms/step - binary_accuracy: 0.7811 - loss: 0.8348 - val_binary_accuracy: 0.7463 - val_loss: 1.6123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dname, train, validation, test in dataset:\n",
    "    for cname, classification in classification_scheme:\n",
    "        for wname, wmodel in word_model:\n",
    "            print(f\"Training {dname}_{wname}_{cname} model\")\n",
    "            train_model(\n",
    "                word=wmodel,\n",
    "                classification=classification,\n",
    "                train_dataset_filename=train,\n",
    "                validation_dataset_filename=validation,\n",
    "                name=f\"{dname}_{wname}_{cname}\",\n",
    "            )\n",
    "            print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
